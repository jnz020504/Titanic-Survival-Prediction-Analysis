# Titanic Survival Prediction Analysis

## Data Preprocessing

The Titanic dataset contains 891 passenger records with features such as Passenger class (`Pclass`), Sex, Age, Fare, etc., and the target `Survived` (0 = No, 1 = Yes). Before modeling, we performed several preprocessing steps:

- **Handling Missing Values:** The dataset has notable missing values in the `Age` (177 missing) and `Cabin` (687 missing) columns, and 2 missing values in `Embarked`. We filled in **Age** with the median age from the training data to avoid biasing with extreme values. The **Embarked** (port of embarkation) missing entries were filled with the mode ('S'), since 'S' is the most common embarkation port. The **Cabin** feature was mostly missing, so rather than attempting to impute specific cabin values, we engineered a binary feature **HasCabin** indicating whether a passenger has a cabin number or not (1 if Cabin is present, 0 if missing). The original `Cabin` column was then dropped.

- **Feature Engineering:** We created a new feature **FamilySize** by combining `SibSp` (number of siblings/spouses aboard) and `Parch` (number of parents/children aboard) as `FamilySize = SibSp + Parch`. This feature captures the total family members traveling with a passenger. A larger family might impact survival (for example, having family could either help or hinder survival). We also considered the passenger class (`Pclass`) as a categorical feature rather than numeric; although `Pclass` is ordinal (1st, 2nd, 3rd class), treating it categorically can capture non-linear effects between classes. The `Name`, `Ticket`, and `PassengerId` columns were dropped as they are not directly useful for prediction (name and ticket are mostly unique identifiers, though names could be used to extract titles, we did not pursue that here).

- **Encoding Categorical Variables:** Categorical features like **Sex**, **Embarked**, and **Pclass** were converted into numerical form. **Sex** is binary, so we encoded it as 0 for male and 1 for female. For **Embarked** (C/Q/S) and **Pclass** (1/2/3), we applied one-hot encoding. To avoid redundant information, we dropped one dummy variable from each (e.g., for `Embarked` we keep dummy columns for Q and S, and for `Pclass` we keep 2 and 3), using the dropped category as the reference. This results in new indicator columns like `Pclass_2`, `Pclass_3`, `Embarked_Q`, and `Embarked_S` (with `Pclass_1` and `Embarked_C` being the implicit reference categories). 

After these steps, our feature set includes: **Sex, Age, Fare, FamilySize, HasCabin, Pclass_2, Pclass_3, Embarked_Q, Embarked_S**. We standardized the numeric features (like Age and Fare) when needed for certain models (e.g., the neural network) to improve training stability. The data was then split into a training set and a hold-out test set (we used 80% for training and 20% for testing) to evaluate model performance on unseen data.

## Model Training and Comparison

We approached the survival prediction as a binary classification task. We trained a variety of models, starting with simple baseline models and then more complex models, to compare their performance:

- **Baseline Models:**  
  - **Logistic Regression:** A linear model that predicts the log-odds of survival as a linear combination of the features. This serves as a simple benchmark and is easy to interpret. We used an L2-regularized logistic regression and increased the maximum iterations to ensure convergence (since some features were not scaled, we allowed the solver more iterations).  
  - **Decision Tree:** A simple decision tree classifier that splits the data based on feature values. We let the tree grow to full depth (no explicit pruning) as a baseline, which risks overfitting but shows the maximum it can fit the training data. The tree can capture non-linear relationships between features and survival (e.g., it can learn rules like "if Sex is female and Pclass is 1 then survive"). 

- **Improved Models:**  
  - **Random Forest:** An ensemble of many decision trees (we used 100 trees by default). Each tree is trained on a bootstrap sample of the data and uses a subset of features for splitting at each node. The forest aggregates the predictions of the trees (majority vote) to improve generalization. This typically yields higher accuracy than a single decision tree by reducing overfitting through averaging.  
  - **XGBoost (Extreme Gradient Boosting):** A gradient-boosted decision tree model. Trees are added sequentially, each trying to correct the errors of the previous ensemble. XGBoost includes regularization and efficient handling of missing values and often achieves state-of-the-art performance in classification tasks. We used the XGBoost classifier with default parameters for boosting (with 100 trees) to evaluate its performance.  
  - **Neural Network (MLP):** A simple **Multi-Layer Perceptron** using a feed-forward neural network with one or two hidden layers. We implemented an MLP with two hidden layers (e.g., 16 and 8 neurons respectively) with ReLU activations. The output layer is a single neuron with a sigmoid activation to produce a probability of survival. We trained the network using the Adam optimizer on binary cross-entropy loss. Given the small dataset, we used early stopping to prevent overfitting (stopping training when validation loss stopped improving). This MLP serves as a simple deep learning approach to see if it can capture patterns beyond what the other models do.

Throughout training, we used the training set for learning the model parameters, and we retained a separate test set to evaluate performance. No model hyperparameter tuning or extensive cross-validation was performed due to the scope of the task, but each model was trained with reasonable default settings. The logistic regression and neural network models benefited from feature scaling, whereas the tree-based models (decision tree, random forest, XGBoost) can naturally handle unscaled feature ranges. 

## Model Evaluation

We evaluated all models on the same held-out test set using several metrics: **Accuracy** (the proportion of correct predictions), **F1 Score** (the harmonic mean of precision and recall for the positive class, which in this context is "survived"), and **ROC-AUC** (Area Under the Receiver Operating Characteristic Curve, measuring the trade-off between true positive rate and false positive rate across thresholds). These metrics provide a balanced view of model performance (accuracy for overall correctness, F1 for how well the model balances false positives vs. false negatives, and AUC for discriminative ability regardless of threshold).

**Test Set Performance:** The table below summarizes the performance of each model on the test data:

| Model                | Accuracy | F1 Score | ROC-AUC |
|----------------------|----------|----------|---------|
| Logistic Regression  | 0.80     | 0.73     | 0.83    |
| Decision Tree        | 0.80     | 0.73     | 0.77    |
| Random Forest        | 0.82     | 0.76     | 0.84    |
| XGBoost              | 0.80     | 0.71     | 0.84    |
| Neural Network (MLP) | 0.78     | 0.70     | 0.83    |

*Key observations:* The baseline logistic regression already achieves ~80% accuracy on the test set. The decision tree, interestingly, obtained a similar accuracy and F1 as logistic regression in this case. This particular tree model appears to have learned patterns that align closely with the logistic model (in fact, it ended up making very similar predictions on the test set, likely by focusing on the most important features like Sex and Fare). However, its ROC-AUC is a bit lower (0.77) indicating that its ranking of positive vs. negative cases is not as refined as the logistic regression's (the tree outputs more extreme probability estimates, being a non-probabilistic model without calibration).

Among the improved models, the **Random Forest** performed the best overall with about 82% accuracy and the highest F1 score (~0.76). It also had one of the highest AUC (~0.84), indicating a strong ability to distinguish survivors from non-survivors. The ensemble of trees clearly improved generalization over a single tree by reducing overfit and capturing more robust patterns.

**XGBoost** achieved a similar ROC-AUC (~0.84) to Random Forest, but its accuracy (~80%) and F1 (~0.71) were a bit lower in our test. This might be due to default parameters not being fully optimized for this dataset or simply random variation, but it still performed on par with logistic regression. With tuning (e.g., more trees or a different learning rate), XGBoost could potentially surpass the random forest. In general, both ensemble methods show an improvement over the baseline models in terms of AUC and maintaining good F1.

The **Neural Network (MLP)** reached around 78% accuracy, slightly below the logistic regression. Its F1 (0.70) and AUC (0.83) suggest it was in the same ballpark as the simpler models but did not outperform the random forest. On such a small dataset, a complex model like an MLP can be harder to train effectively â€“ it may require more tuning or may simply not have enough data to beat tree ensembles that are more straightforward for tabular data. Our MLP likely needed more optimization or could benefit from techniques like cross-validation or more features (such as extracting title from name or others) to really shine. As it stands, the MLP gave a reasonable performance but did not yield improvement over the simpler approaches.
![image](https://github.com/user-attachments/assets/a16f8e9b-a33d-4ae1-9b73-8753e5e05abe)

*Confusion matrices for each model on the test set.* Each matrix shows the count of true negatives (top-left), false positives (top-right), false negatives (bottom-left), and true positives (bottom-right). We see that all models correctly classify a large number of the negative cases (passengers who died, shown as true negatives in the top-left of each matrix). The **Logistic Regression** and **Decision Tree** (top row) misclassified 21 survivors as deceased (false negatives) and 14 deceased as survivors (false positives). The **Random Forest** (top right) improved slightly, with fewer false negatives (17) and a bit more false positives (15), thereby correctly identifying more survivors (52 true positives versus 48 in logistic regression). **XGBoost** (bottom left) in our evaluation had 44 true positives and 25 false negatives â€“ it missed more survivors than the random forest, aligning with its lower F1 score. The **Neural Network** (bottom right) had 45 true positives and 24 false negatives, also a bit worse than logistic regression. These confusion matrices highlight the trade-offs: models like random forest achieved higher true positives (recall) at the cost of a few more false positives, which is often acceptable when the goal is to save lives (identifying as many survivors as possible might be prioritized over falsely tagging someone as survived).
![image](https://github.com/user-attachments/assets/2f2f8a34-ca4a-431e-b3d6-abbf36a95aba)

*ROC Curves for the models.* The Receiver Operating Characteristic curves illustrate the true positive rate vs. false positive rate for all classification thresholds. A model that perfectly separates survivors and non-survivors would reach the top-left corner (TPR=1, FPR=0). The black dashed line is the baseline of random guessing (AUC = 0.50). We can see that all models perform well above chance. The **Random Forest** and **XGBoost** (pink and red lines) achieved the highest curves, overlapping significantly in the upper region (AUC ~0.84). The **Logistic Regression** and **Neural Network** (yellow and blue lines) are very close to each other with AUC ~0.83, and the **Decision Tree** (orange line) is a bit lower (AUC ~0.77), indicating it was the weakest in ranking positive cases. Overall, the ROC curves reinforce that the ensemble models have a slight edge in distinguishing survivors, but logistic regression was not far behind. The similar performance of logistic regression is notable â€“ it suggests that the relationship between features and survival might be mostly linear or that the engineered features captured much of the signal (especially Sex, Fare, Age), allowing a simple model to do well. The tree-based models, however, may capture some non-linear interactions (for instance, perhaps different thresholds of Age or Fare for different classes).

## Feature Importance

For the tree-based models, we can inspect feature importance scores to understand which features were most influential in the survival predictions:
![image](https://github.com/user-attachments/assets/b4cecd49-9f75-48ba-b25f-6286ccca8e04)

*Feature importance derived from the Random Forest model.* Higher scores indicate a greater contribution of that feature to the modelâ€™s decisions (averaged over all trees). We see **Sex, Fare, and Age** are by far the most important features. In fact, *Sex (gender)* is the top feature â€“ this aligns with the historical reality that women had a much higher survival rate (the "women and children first" policy). The model heavily uses whether the passenger is female or male to make splits. **Fare** is the next most important; passengers who paid higher fares (often a proxy for **Pclass** 1, i.e., first-class tickets) had better accommodations and likely better access to lifeboats, thus higher survival chances. **Age** is also very important â€“ younger passengers (especially children) had higher priority in rescue, whereas older passengers were less likely to survive. 

There is a sizeable drop-off after the top three: **FamilySize** and **HasCabin** have some importance but comparatively lower. FamilySize being somewhat important could mean the model found patterns like very large families had lower survival (since it could be harder to get everyone on a lifeboat) or that being alone vs. with family affected survival chances. The **HasCabin** feature may partly capture the effect of passenger class as well â€“ those with a cabin (recorded) tended to be in higher classes. The dummy variables for **Pclass** (2nd or 3rd class) and **Embarked** (port Q or S vs. C) showed relatively low importance in the random forest. This is likely because the information from these is largely subsumed by other features: for example, **Fare** correlates with Pclass (first-class tickets are expensive, third-class are cheap), and **Embarked** has a minor influence (passengers from different embarkation ports had different demographics, but itâ€™s a weaker factor for survival than socio-economic status or gender). 

The XGBoost model (not shown in the chart) exhibited a very similar importance pattern â€“ it also found Sex, Fare, and Age to be top predictors of survival. This consistency across models provides confidence in these insights: **being female, of higher socio-economic status (as indicated by fare/class), and younger were strong predictors of survival on the Titanic.**

## Conclusion

In this analysis, we built and evaluated multiple models to predict Titanic passenger survival. Through data preprocessing (handling missing ages, encoding categorical gender/embarkation, creating family size, etc.), we prepared a feature set that allowed even simple models to perform relatively well. Among the models, ensemble methods like Random Forest and XGBoost delivered the best performance (around 82% accuracy and highest AUC ~0.84), outperforming the baseline Logistic Regression (~80% accuracy) and the single Decision Tree. The neural network did not outperform the tree ensembles, likely due to the limited data size and the strong signal captured by a few key features that the other models already exploited.

Crucially, the models agree on what drives survival outcomes: being a woman and/or a child (lower age), and being in a higher class (or paying a higher fare, having a cabin) significantly increase the chances of survival. These findings echo the well-known historical facts of the Titanic disaster. The exercise demonstrates how even basic machine learning models can uncover patterns that align with domain knowledge. It also shows that more complex models can provide marginal gains in predictive performance, but understanding feature importance and the underlying story is equally important. 
